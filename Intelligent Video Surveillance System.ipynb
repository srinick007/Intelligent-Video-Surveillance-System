{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19ba6e2c-713d-4d8a-ae42-b81ede5b8df7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'get_anchors_coordinates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# call the function with the specified model\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mprocess_webcam_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 39\u001b[0m, in \u001b[0;36mprocess_webcam_feed\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# annotate the frame with inference results\u001b[39;00m\n\u001b[0;32m     38\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m bounding_box_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mframe, detections\u001b[38;5;241m=\u001b[39mdetections)\n\u001b[1;32m---> 39\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_annotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotated_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mperson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# display the annotated frame\u001b[39;00m\n\u001b[0;32m     42\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebcam Feed\u001b[39m\u001b[38;5;124m'\u001b[39m, annotated_frame)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clg\\Lib\\site-packages\\supervision\\annotators\\core.py:971\u001b[0m, in \u001b[0;36mLabelAnnotator.annotate\u001b[1;34m(self, scene, detections, labels, custom_color_lookup)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;124;03mAnnotates the given scene with labels based on the provided detections.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124;03msupervision-annotator-examples/label-annotator-example-purple.png)\u001b[39;00m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    970\u001b[0m font \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX\n\u001b[1;32m--> 971\u001b[0m anchors_coordinates \u001b[38;5;241m=\u001b[39m \u001b[43mdetections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_anchors_coordinates\u001b[49m(\n\u001b[0;32m    972\u001b[0m     anchor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_anchor\n\u001b[0;32m    973\u001b[0m )\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(detections):\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of labels provided (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of detections (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(detections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Each detection should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength as the Detections object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    982\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'get_anchors_coordinates'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from inference import get_model\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import winsound  # for playing sound on Windows\n",
    "\n",
    "# function to perform inference on webcam frames\n",
    "def process_webcam_feed():\n",
    "    # load a pre-trained yolov8n model\n",
    "    model = get_model(model_id=\"people-detection-jhhbd/1\")\n",
    "\n",
    "    # create supervision annotators\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    # open webcam\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # run inference on the frame\n",
    "        results = model.infer(frame)\n",
    "\n",
    "        # load the results into the supervision Detections api\n",
    "        detections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n",
    "\n",
    "        # check if 'people detection - v4 2023-11-04 9-25pm' class is detected\n",
    "        for detection in detections:\n",
    "\n",
    "            if detection[5]['class_name'] == 'people detection - v4 2023-11-04 9-25pm':\n",
    "                # Play alarm sound\n",
    "                winsound.PlaySound('alarm.mp3', winsound.SND_ASYNC | winsound.SND_LOOP)\n",
    "                break\n",
    "    \n",
    "        # annotate the frame with inference results\n",
    "        annotated_frame = bounding_box_annotator.annotate(scene=frame, detections=detections)\n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detection,labels=[\"person\"])\n",
    "\n",
    "        # display the annotated frame\n",
    "        cv2.imshow('Webcam Feed', annotated_frame)\n",
    "\n",
    "        # Press 'q' to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# call the function with the specified model\n",
    "process_webcam_feed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354375f-0a0f-47d5-9d58-80bf8b2cf31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
